{"id":"auto-commentary-0yr","title":"Typography polish: line spacing, column gaps, consistent punctuation","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-09T16:59:02.117471-06:00","updated_at":"2026-01-09T16:59:02.117471-06:00"}
{"id":"auto-commentary-3di","title":"Update Gloss model to consume NormalizedLexicalEntry","description":"## Task\n\nUpdate the `Gloss` model in `autocom/core/models.py` to work seamlessly with NormalizedLexicalEntry.\n\n## Current Gloss Model\n\n```python\nclass Gloss(BaseModel):\n    lemma: str\n    senses: List[str] = []\n    headword: Optional[str] = None\n    genitive: Optional[str] = None\n    gender: Optional[str] = None\n    pos_abbrev: Optional[str] = None\n    principal_parts: Optional[str] = None\n    frequency: Optional[int] = None\n```\n\n## Changes Needed\n\n### 1. Add Factory Method\n\n```python\nclass Gloss(BaseModel):\n    # ... existing fields ...\n    \n    # New fields for provenance\n    source: Optional[str] = None\n    confidence: Optional[float] = None\n    \n    @classmethod\n    def from_normalized_entry(\n        cls,\n        entry: NormalizedLexicalEntry,\n        frequency: Optional[int] = None,\n    ) -\u003e \"Gloss\":\n        \"\"\"Create Gloss from a normalized lexical entry.\n        \n        This is the preferred way to create Gloss instances, ensuring\n        consistent transformation from the canonical data model.\n        \"\"\"\n        # Map POS enum to abbreviation string\n        pos_abbrev = POS_ABBREV_MAP.get(entry.pos)\n        \n        # Map Gender enum to abbreviation string\n        gender = GENDER_ABBREV_MAP.get(entry.gender) if entry.gender else None\n        \n        # Format principal parts as string\n        principal_parts = None\n        if entry.principal_parts:\n            parts_str = \", \".join(entry.principal_parts)\n            if entry.conjugation:\n                parts_str += f\" ({entry.conjugation})\"\n            principal_parts = parts_str\n        \n        return cls(\n            lemma=entry.lemma,\n            senses=entry.senses,\n            headword=entry.headword,\n            genitive=entry.genitive,\n            gender=gender,\n            pos_abbrev=pos_abbrev,\n            principal_parts=principal_parts,\n            frequency=frequency or entry.frequency,\n            source=entry.source,\n            confidence=entry.confidence,\n        )\n```\n\n### 2. Add Helper Property\n\n```python\n@property\ndef best(self) -\u003e Optional[str]:\n    \"\"\"Get the best (first) definition, or None if no definitions.\"\"\"\n    return self.senses[0] if self.senses else None\n```\n\n### 3. POS Abbreviation Mapping\n\n```python\nPOS_ABBREV_MAP = {\n    PartOfSpeech.NOUN: None,  # Gender suffices\n    PartOfSpeech.VERB: \"v.\",\n    PartOfSpeech.ADJECTIVE: \"adj.\",\n    PartOfSpeech.ADVERB: \"adv.\",\n    PartOfSpeech.PREPOSITION: \"prep.\",\n    PartOfSpeech.CONJUNCTION: \"conj.\",\n    PartOfSpeech.PRONOUN: \"pron.\",\n    PartOfSpeech.INTERJECTION: \"interj.\",\n    PartOfSpeech.NUMERAL: \"num.\",\n    PartOfSpeech.PARTICLE: \"part.\",\n    PartOfSpeech.ARTICLE: \"art.\",\n}\n\nGENDER_ABBREV_MAP = {\n    Gender.MASCULINE: \"m.\",\n    Gender.FEMININE: \"f.\",\n    Gender.NEUTER: \"n.\",\n    Gender.COMMON: \"c.\",\n}\n```\n\n## Template Compatibility\n\nEnsure LaTeX templates still work:\n- `{{ token.gloss.headword }}`\n- `{{ token.gloss.best }}`\n- `{{ token.gloss.genitive }}`\n- `{{ token.gloss.gender }}`\n- `{{ token.gloss.pos_abbrev }}`\n- `{{ token.gloss.principal_parts }}`\n\n## Test Cases\n\n- Create Gloss from Latin noun entry\n- Create Gloss from Latin verb entry\n- Create Gloss from Greek entry with article\n- Verify `best` property works\n- Verify template-used fields populated correctly\n- Test source and confidence passed through\n\n## Acceptance Criteria\n\n- [ ] Factory method implemented\n- [ ] POS and gender mapping works\n- [ ] Principal parts formatted correctly\n- [ ] Source/confidence fields added\n- [ ] Backward compatible with existing code\n- [ ] Templates still render correctly","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:13:33.253345-06:00","updated_at":"2026-01-11T10:13:33.253345-06:00","dependencies":[{"issue_id":"auto-commentary-3di","depends_on_id":"auto-commentary-4d5","type":"blocks","created_at":"2026-01-11T10:15:16.971402-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-3v7","title":"Use document title in header instead of generic Commentary","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-09T16:59:01.84366-06:00","updated_at":"2026-01-09T16:59:01.84366-06:00"}
{"id":"auto-commentary-4d5","title":"Define core normalization data models","description":"## Task\n\nCreate the core data models for the normalization layer in a new module `autocom/core/lexical.py`.\n\n## Models to Create\n\n### 1. Enums\n\n```python\nfrom enum import Enum\n\nclass Language(Enum):\n    LATIN = \"latin\"\n    GREEK = \"greek\"\n\nclass PartOfSpeech(Enum):\n    NOUN = \"noun\"\n    VERB = \"verb\"\n    ADJECTIVE = \"adjective\"\n    ADVERB = \"adverb\"\n    PREPOSITION = \"preposition\"\n    CONJUNCTION = \"conjunction\"\n    PRONOUN = \"pronoun\"\n    INTERJECTION = \"interjection\"\n    NUMERAL = \"numeral\"\n    PARTICLE = \"particle\"\n    ARTICLE = \"article\"  # Greek only\n    UNKNOWN = \"unknown\"\n\nclass Gender(Enum):\n    MASCULINE = \"m\"\n    FEMININE = \"f\"\n    NEUTER = \"n\"\n    COMMON = \"c\"  # Can be M or F\n    UNKNOWN = \"x\"\n\nclass Number(Enum):\n    SINGULAR = \"sg\"\n    PLURAL = \"pl\"\n    DUAL = \"du\"  # Greek only\n    PLURAL_ONLY = \"pl_tantum\"  # arma, castra, Athenae\n```\n\n### 2. NormalizedLexicalEntry (Pydantic model)\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\nclass NormalizedLexicalEntry(BaseModel):\n    \"\"\"Canonical internal representation of a dictionary entry.\n    \n    This is the single source of truth for lexical data after extraction\n    from any dictionary source. All rendering should consume this model.\n    \"\"\"\n    # Core identification\n    headword: str = Field(..., description=\"Full dictionary form with diacritics\")\n    lemma: str = Field(..., description=\"Normalized lookup key (lowercase, no diacritics)\")\n    language: Language\n    pos: PartOfSpeech\n    \n    # Definitions\n    senses: List[str] = Field(default_factory=list, description=\"Cleaned definitions\")\n    \n    # Nominal morphology (nouns, adjectives, pronouns)\n    gender: Optional[Gender] = None\n    declension: Optional[int] = Field(None, ge=1, le=5, description=\"Declension class\")\n    genitive: Optional[str] = Field(None, description=\"Genitive ending, e.g. '-ae'\")\n    article: Optional[str] = Field(None, description=\"Greek article: ὁ, ἡ, τό\")\n    \n    # Verbal morphology\n    conjugation: Optional[int] = Field(None, ge=1, le=4, description=\"Latin conjugation\")\n    principal_parts: Optional[List[str]] = Field(None, description=\"Verb principal parts\")\n    \n    # Metadata\n    source: str = Field(..., description=\"Dictionary source identifier\")\n    confidence: float = Field(1.0, ge=0.0, le=1.0, description=\"Match quality score\")\n    frequency: Optional[int] = Field(None, description=\"Occurrence count in text\")\n    is_proper_noun: bool = False\n    variant_of: Optional[str] = Field(None, description=\"If spelling variant, canonical form\")\n    \n    class Config:\n        use_enum_values = True\n```\n\n### 3. RawEntry Protocol (for type hints)\n\n```python\nfrom typing import Protocol, Any, Dict\n\nclass RawLexicalEntry(Protocol):\n    \"\"\"Protocol for raw entries from any dictionary source.\"\"\"\n    \n    def to_dict(self) -\u003e Dict[str, Any]:\n        \"\"\"Convert to dictionary for debugging/logging.\"\"\"\n        ...\n```\n\n## File Location\n\n`autocom/core/lexical.py`\n\n## Tests\n\nCreate `tests/core/test_lexical.py`:\n- Test enum values\n- Test NormalizedLexicalEntry validation\n- Test required vs optional fields\n- Test confidence bounds\n- Test serialization/deserialization\n\n## Acceptance Criteria\n\n- [ ] All enums defined with appropriate values\n- [ ] NormalizedLexicalEntry validates correctly\n- [ ] Model can represent both Latin and Greek entries\n- [ ] Unit tests pass\n- [ ] Type hints work correctly","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-11T10:11:25.826481-06:00","updated_at":"2026-01-11T11:02:16.804009-06:00","closed_at":"2026-01-11T11:02:16.804009-06:00","close_reason":"Complete: core normalization data models with Morpheus parity (stemtype, stem/suffix decomposition, dialect tracking, POS ordering)"}
{"id":"auto-commentary-8g3","title":"Implement Latin Lewis \u0026 Short normalizer","description":"## Task\n\nCreate a normalizer that converts raw Lewis \u0026 Short JSON entries to NormalizedLexicalEntry.\n\n## Location\n\n`autocom/languages/latin/normalizers/lewis_short.py`\n\n## Interface\n\n```python\nfrom typing import Optional, Dict, Any\nfrom autocom.core.lexical import NormalizedLexicalEntry\n\nclass LewisShortNormalizer:\n    \"\"\"Normalizes Lewis \u0026 Short dictionary entries to canonical form.\"\"\"\n    \n    def normalize(self, entry: Dict[str, Any], query_lemma: str) -\u003e Optional[NormalizedLexicalEntry]:\n        \"\"\"Convert Lewis \u0026 Short entry to normalized form.\n        \n        Args:\n            entry: Raw L\u0026S JSON entry (from ls_*.json files)\n            query_lemma: The lemma used for lookup\n            \n        Returns:\n            NormalizedLexicalEntry if successful, None if invalid\n        \"\"\"\n        ...\n```\n\n## Lewis \u0026 Short Entry Structure\n\nThe L\u0026S JSON entries have this structure:\n```json\n{\n  \"key\": \"amo\",\n  \"title_orthography\": \"ămo\",\n  \"title_genitive\": null,\n  \"gender\": null,\n  \"part_of_speech\": \"verb\",\n  \"main_notes\": \"āmō, āvī, ātum, 1\",\n  \"senses\": [\"to love\", \"to be fond of\", ...]\n}\n```\n\n## Key Responsibilities\n\n### 1. Headword Extraction\n\nL\u0026S provides headwords directly - use `title_orthography` (includes macrons) or `key`.\n\n```python\nheadword = entry.get(\"title_orthography\") or entry.get(\"key\", \"\")\n```\n\n### 2. POS Mapping\n\n```python\nLS_POS_MAP = {\n    \"noun\": PartOfSpeech.NOUN,\n    \"verb\": PartOfSpeech.VERB,\n    \"adjective\": PartOfSpeech.ADJECTIVE,\n    \"adverb\": PartOfSpeech.ADVERB,\n    \"preposition\": PartOfSpeech.PREPOSITION,\n    \"conjunction\": PartOfSpeech.CONJUNCTION,\n    \"pronoun\": PartOfSpeech.PRONOUN,\n    \"interjection\": PartOfSpeech.INTERJECTION,\n    \"numeral\": PartOfSpeech.NUMERAL,\n    \"particle\": PartOfSpeech.PARTICLE,\n}\n```\n\n### 3. Gender Mapping\n\n```python\nLS_GENDER_MAP = {\n    \"M\": Gender.MASCULINE,\n    \"F\": Gender.FEMININE,\n    \"N\": Gender.NEUTER,\n    \"C\": Gender.COMMON,\n}\n```\n\n### 4. Genitive Extraction\n\nFormat `title_genitive` as \"-ending\":\n```python\ngenitive = entry.get(\"title_genitive\")\nif genitive and genitive != \"indecl.\":\n    genitive = f\"-{genitive}\" if not genitive.startswith(\"-\") else genitive\n```\n\n### 5. Principal Parts Extraction\n\nParse `main_notes` for verbs:\n```\n\"āmō, āvī, ātum, 1\" -\u003e [\"āvī\", \"ātum\"], conjugation=1\n```\n\n### 6. Sense Cleaning\n\nL\u0026S senses often contain scholarly apparatus - clean for pedagogical use:\n- Remove author citations (Cic., Verg., etc.)\n- Remove cross-references\n- Simplify technical grammar notes\n- Extract core English meaning\n\n## Test Cases\n\n- Verb with principal parts\n- Noun with genitive and gender\n- Adjective\n- Entry with macrons preserved\n- Entry with complex senses (test cleaning)\n\n## Acceptance Criteria\n\n- [ ] Headword with macrons preserved\n- [ ] POS correctly mapped\n- [ ] Gender correctly extracted\n- [ ] Genitive formatted consistently\n- [ ] Principal parts parsed from main_notes\n- [ ] Senses appropriately cleaned\n- [ ] All test cases pass","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:12:33.596894-06:00","updated_at":"2026-01-11T10:12:33.596894-06:00","dependencies":[{"issue_id":"auto-commentary-8g3","depends_on_id":"auto-commentary-4d5","type":"blocks","created_at":"2026-01-11T10:15:16.660473-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-98t","title":"Fix Whitaker's Words headword construction for non-verbs","description":"## Problem\n\nWhitaker's Words returns morphological **stems**, not dictionary headwords. The lexicon code correctly handles verbs by adding conjugation endings, but for nouns, adjectives, and pronouns it just uses the raw stem.\n\n**Affected code**: `autocom/languages/latin/lexicon.py` lines 981-994\n\n## Status Update\n\n**This issue is superseded by the Normalization Layer Epic (auto-commentary-k4f).**\n\nThe fix for this bug is being implemented as part of auto-commentary-m7s (Implement Latin Whitaker's Words normalizer), which provides a proper architectural solution rather than a point fix.\n\nThe sub-tasks (fq9, nmj, c0y) for noun/adjective/pronoun headword reconstruction will be incorporated into the normalizer implementation.\n\n## Original Analysis\n\n| Word | Stem returned | Should be | Type |\n|------|---------------|-----------|------|\n| multum | `mult` | multus | adj 1st/2nd decl |\n| ille | `ill` | ille | pronoun |\n| Italiam | `itali` | Italia | noun 1st decl F |\n| arma | `arm` | arma | noun 2nd decl N plural tantum |\n| terra | `terr` | terra | noun 1st decl F |\n| saevae | `saev` | saevus | adj 1st/2nd decl |\n| primum | `prim` | primus | adj 1st/2nd decl |","status":"open","priority":1,"issue_type":"bug","created_at":"2026-01-11T10:04:22.678509-06:00","updated_at":"2026-01-11T10:18:38.529678-06:00","dependencies":[{"issue_id":"auto-commentary-98t","depends_on_id":"auto-commentary-fq9","type":"blocks","created_at":"2026-01-11T10:05:52.100957-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-98t","depends_on_id":"auto-commentary-nmj","type":"blocks","created_at":"2026-01-11T10:05:52.262314-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-98t","depends_on_id":"auto-commentary-c0y","type":"blocks","created_at":"2026-01-11T10:05:52.429198-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-alf","title":"Implement Greek LSJ/Morpheus normalizer","description":"## Task\n\nCreate normalizers for Greek dictionary sources (LSJ, Morpheus/Perseus API, CLTK).\n\n## Location\n\n`autocom/languages/greek/normalizers/`\n- `lsj.py` - Liddell-Scott-Jones\n- `morpheus.py` - Perseus/Morpheus API\n- `cltk.py` - CLTK lemmatizer output\n\n## Greek-Specific Considerations\n\n### 1. Headword Forms\n\nGreek dictionary conventions:\n- **Nouns**: Nominative singular + article (ὁ λόγος, ἡ ψυχή, τό ἔργον)\n- **Verbs**: 1st person singular present indicative active (λύω, παιδεύω)\n- **Adjectives**: Masculine nominative singular (ἀγαθός, σοφός)\n\n### 2. Articles\n\nGreek nouns require articles to indicate gender:\n```python\nGREEK_ARTICLES = {\n    Gender.MASCULINE: \"ὁ\",\n    Gender.FEMININE: \"ἡ\",\n    Gender.NEUTER: \"τό\",\n}\n```\n\n### 3. Principal Parts (6 for Greek!)\n\nGreek verbs have up to 6 principal parts:\n1. Present active (λύω)\n2. Future active (λύσω)\n3. Aorist active (ἔλυσα)\n4. Perfect active (λέλυκα)\n5. Perfect middle/passive (λέλυμαι)\n6. Aorist passive (ἐλύθην)\n\n```python\nprincipal_parts: List[str] = [\n    future_active,      # λύσω\n    aorist_active,      # ἔλυσα  \n    perfect_active,     # λέλυκα\n    perfect_mid_pass,   # λέλυμαι\n    aorist_passive,     # ἐλύθην\n]\n```\n\n### 4. Declension Patterns\n\nGreek has 3 declensions with many sub-patterns:\n- 1st: -α, -η (mostly feminine)\n- 2nd: -ος, -ον (masculine, neuter)\n- 3rd: consonant stems (various)\n\n### 5. Accent Handling\n\nGreek accents are semantically significant - must preserve:\n- Acute (ά)\n- Grave (ὰ)\n- Circumflex (ᾶ)\n- Breathing marks (ἁ, ἀ)\n- Iota subscript (ᾳ)\n\n### 6. Beta Code Conversion\n\nSome sources use Beta Code - need conversion:\n```\na)gaqo/s -\u003e ἀγαθός\nlu/w -\u003e λύω\n```\n\n## LSJ Entry Structure\n\nIf using digitized LSJ (e.g., Perseus):\n```json\n{\n  \"lemma\": \"λύω\",\n  \"pos\": \"verb\",\n  \"definitions\": [\"loose\", \"release\", \"dissolve\"],\n  \"forms\": {...}\n}\n```\n\n## Morpheus API Response\n\n```json\n{\n  \"word\": \"λύει\",\n  \"lemma\": \"λύω\",\n  \"pos\": \"verb\",\n  \"person\": \"3rd\",\n  \"number\": \"singular\",\n  \"tense\": \"present\",\n  \"mood\": \"indicative\",\n  \"voice\": \"active\"\n}\n```\n\n## Test Cases\n\n- Noun with article: ὁ λόγος, ἡ ψυχή, τό ἔργον\n- Verb with principal parts: λύω, παιδεύω\n- Contract verb: τιμάω, ποιέω\n- -μι verb: δίδωμι, τίθημι\n- Adjective: ἀγαθός, σοφός\n- 3rd declension noun: ὁ ἀνήρ (ἀνδρός)\n- Proper accent preservation\n\n## Acceptance Criteria\n\n- [ ] Greek headwords correctly formed\n- [ ] Articles assigned based on gender\n- [ ] All 6 principal parts extracted where available\n- [ ] Accents and breathing marks preserved\n- [ ] Beta code conversion if needed\n- [ ] POS correctly mapped\n- [ ] Test cases pass","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:12:55.739575-06:00","updated_at":"2026-01-11T10:12:55.739575-06:00","dependencies":[{"issue_id":"auto-commentary-alf","depends_on_id":"auto-commentary-4d5","type":"blocks","created_at":"2026-01-11T10:15:16.816252-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-c0y","title":"Add pronoun headword special cases","description":"## Task\n\nPronouns need special handling because they don't follow regular declension patterns. Create a mapping from stem to dictionary headword.\n\n## Mapping Logic\n\n```python\n# stem -\u003e dictionary headword for common pronouns\nPRON_HEADWORD_MAP = {\n    'ill': 'ille',        # ille, illa, illud\n    'hic': 'hic',         # hic, haec, hoc (stem varies: h-)\n    'h': 'hic',           # alternate stem\n    'is': 'is',           # is, ea, id\n    'e': 'is',            # alternate stem for ea\n    'qui': 'qui',         # qui, quae, quod\n    'qu': 'qui',          # alternate stem\n    'quis': 'quis',       # quis, quid\n    'ips': 'ipse',        # ipse, ipsa, ipsum\n    'idem': 'idem',       # idem, eadem, idem\n    'eid': 'idem',        # alternate stem\n    'nos': 'nos',         # personal pronouns\n    'vos': 'vos',\n    'ego': 'ego',\n    'tu': 'tu',\n    'su': 'sui',          # reflexive\n    'ali': 'alius',       # alius, alia, aliud\n    'alt': 'alter',       # alter, altera, alterum\n    'uter': 'uter',       # uter, utra, utrum\n    'neuter': 'neuter',   # neuter, neutra, neutrum\n    'null': 'nullus',     # nullus, nulla, nullum\n    'tot': 'totus',       # totus, tota, totum\n    'sol': 'solus',       # solus, sola, solum (pronominal adj)\n}\n```\n\n## Implementation\n\nIn `_lookup_whitaker_with_metadata`:\n\n```python\nif wt_name == 'PRON':\n    stem_lower = stem.lower()\n    if stem_lower in PRON_HEADWORD_MAP:\n        result['headword'] = PRON_HEADWORD_MAP[stem_lower]\n    else:\n        # Fallback: try adding common pronoun endings\n        result['headword'] = stem  # May need more logic\n```\n\n## Test Cases\n\n- ill -\u003e ille\n- h/hic -\u003e hic\n- qui/qu -\u003e qui\n- ips -\u003e ipse\n- ego -\u003e ego","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:05:15.658938-06:00","updated_at":"2026-01-11T10:05:15.658938-06:00"}
{"id":"auto-commentary-fq9","title":"Add noun nominative ending reconstruction","description":"## Task\n\nAdd NOUN_NOM_ENDING_MAP to reconstruct nominative singular headwords for nouns from Whitaker's stems.\n\n## Mapping Logic\n\n```python\n# (declension, gender, is_plural_tantum) -\u003e nominative ending\nNOUN_NOM_ENDING_MAP = {\n    # 1st declension\n    (1, 'F', False): 'a',      # terra, via, Italia\n    (1, 'M', False): 'a',      # poeta, nauta (masc 1st decl)\n    \n    # 2nd declension\n    (2, 'M', False): 'us',     # dominus, animus\n    (2, 'N', False): 'um',     # bellum, consilium\n    (2, 'N', True): 'a',       # arma, castra (pluralia tantum)\n    (2, 'M', True): 'i',       # liberi (pluralia tantum)\n    \n    # 3rd declension - complex, may need stem inspection\n    (3, 'M', False): '',       # varies: rex, homo, miles\n    (3, 'F', False): '',       # varies: pax, urbs, lex\n    (3, 'N', False): '',       # varies: corpus, nomen, iter\n    \n    # 4th declension\n    (4, 'M', False): 'us',     # eventus, exercitus\n    (4, 'N', False): 'u',      # cornu, genu\n    \n    # 5th declension\n    (5, 'F', False): 'es',     # res, dies\n}\n```\n\n## Implementation\n\nIn `_lookup_whitaker_with_metadata`, after extracting roots/category/form:\n\n```python\nif wt_name == 'N':\n    decl = category[0] if category else None\n    gender = form_info[0] if form_info else None\n    is_plural = form_info[1] in ('P', 'T') if len(form_info) \u003e 1 else False\n    key = (decl, gender, is_plural)\n    ending = NOUN_NOM_ENDING_MAP.get(key, '')\n    result['headword'] = f'{stem}{ending}'\n```\n\n## Test Cases\n\n- terr + a = terra (1st F)\n- arm + a = arma (2nd N plural tantum)\n- domin + us = dominus (2nd M)\n- bell + um = bellum (2nd N)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:04:52.503037-06:00","updated_at":"2026-01-11T10:04:52.503037-06:00"}
{"id":"auto-commentary-fwp","title":"Line numbers in left margin instead of inline with text","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-09T16:59:00.998836-06:00","updated_at":"2026-01-09T16:59:00.998836-06:00"}
{"id":"auto-commentary-g4a","title":"Exclude words with no definition from glossary and write them to an errors output file for review","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-09T17:00:01.119795-06:00","updated_at":"2026-01-11T09:50:04.007835-06:00","closed_at":"2026-01-11T09:50:04.007835-06:00","close_reason":"Closed"}
{"id":"auto-commentary-k4f","title":"EPIC: Implement Lexical Entry Normalization Layer","description":"## Overview\n\nImplement a normalization layer between raw dictionary data extraction and output rendering. This addresses the fundamental architectural issue where source-specific transformation logic is scattered across the codebase, leading to bugs like truncated headwords (auto-commentary-98t).\n\n## Problem Statement\n\nCurrently, dictionary lookups from different sources (Whitaker's Words, Lewis \u0026 Short, APIs) return data in different formats, and transformation to the output format happens ad-hoc in each lookup method. This causes:\n\n1. **Scattered normalization logic** - headword reconstruction, sense cleaning, POS mapping all done inline\n2. **Inconsistent handling** - some sources get full treatment, others minimal\n3. **Difficult testing** - can't test extraction vs. normalization separately\n4. **Hard to extend** - adding a new source requires understanding all downstream code\n5. **No data provenance** - can't track which source provided what data\n\n## Proposed Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                       RAW DATA EXTRACTION                           │\n│  (Source-specific, preserves original structure)                    │\n├─────────────────────────────────────────────────────────────────────┤\n│ Latin:                          │ Greek:                            │\n│  • WhitakersRawEntry            │  • MorpheusRawEntry               │\n│  • LewisShortRawEntry           │  • LSJRawEntry                    │\n│  • WordNetAPIRawEntry           │  • PerseusAPIRawEntry             │\n│  • LatinSimpleRawEntry          │  • CLTKRawEntry                   │\n└─────────────────────────────────────────────────────────────────────┘\n                                  │\n                                  ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                      NORMALIZATION LAYER                            │\n│  (Language-aware, source-agnostic canonical model)                  │\n├─────────────────────────────────────────────────────────────────────┤\n│ NormalizedLexicalEntry:                                             │\n│   • headword: str (full dictionary form, with diacritics/macrons)  │\n│   • lemma: str (normalized lookup key)                              │\n│   • language: Language (latin/greek enum)                           │\n│   • pos: PartOfSpeech (standardized enum)                          │\n│   • senses: List[str] (cleaned, pedagogical)                       │\n│   • gender: Optional[Gender] (for nouns)                           │\n│   • declension: Optional[int] (noun/adj pattern)                   │\n│   • conjugation: Optional[int] (verb pattern)                      │\n│   • genitive: Optional[str] (noun genitive ending)                 │\n│   • article: Optional[str] (Greek: ὁ/ἡ/τό)                         │\n│   • principal_parts: Optional[List[str]] (verb forms)              │\n│   • source: str (provenance tracking)                              │\n│   • confidence: float (match quality)                              │\n└─────────────────────────────────────────────────────────────────────┘\n                                  │\n                                  ▼\n┌─────────────────────────────────────────────────────────────────────┐\n│                       OUTPUT RENDERING                              │\n│  (Consumes only NormalizedLexicalEntry)                            │\n├─────────────────────────────────────────────────────────────────────┤\n│  • Gloss model (simplified view for templates)                     │\n│  • LaTeX templates                                                  │\n│  • JSON export                                                      │\n│  • Missing definitions report                                       │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\n## Linguistic Design: NormalizedLexicalEntry\n\n### Core Fields (Both Languages)\n\n| Field | Type | Description | Example (Latin) | Example (Greek) |\n|-------|------|-------------|-----------------|-----------------|\n| headword | str | Full dictionary form | \"āmō\" | \"λύω\" |\n| lemma | str | Normalized lookup key | \"amo\" | \"λυω\" |\n| language | Language | Enum | Language.LATIN | Language.GREEK |\n| pos | PartOfSpeech | Standardized enum | POS.VERB | POS.VERB |\n| senses | List[str] | Clean definitions | [\"to love\"] | [\"to loose\"] |\n\n### Nominal Fields (Nouns, Adjectives)\n\n| Field | Type | Latin Example | Greek Example |\n|-------|------|---------------|---------------|\n| gender | Gender | Gender.F | Gender.M |\n| declension | int | 1 (1st decl) | 1 (1st decl) |\n| genitive | str | \"-ae\" | \"-ου\" |\n| article | str | None | \"ὁ\" |\n\n### Verbal Fields\n\n| Field | Type | Latin Example | Greek Example |\n|-------|------|---------------|---------------|\n| conjugation | int | 1 (1st conj) | None (Greek uses patterns) |\n| principal_parts | List[str] | [\"amāvī\", \"amātum\"] | [\"λύσω\", \"ἔλυσα\", \"λέλυκα\", \"λέλυμαι\", \"ἐλύθην\"] |\n\n### Metadata Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| source | str | \"whitakers\", \"lewis_short\", \"lsj\", etc. |\n| confidence | float | 1.0 = exact match, 0.5 = fuzzy, 0.0 = not found |\n| frequency | int | Occurrence count in current text |\n| is_proper_noun | bool | Names, places, etc. |\n\n## Part of Speech Standardization\n\n```python\nclass PartOfSpeech(Enum):\n    NOUN = \"noun\"\n    VERB = \"verb\"\n    ADJECTIVE = \"adjective\"\n    ADVERB = \"adverb\"\n    PREPOSITION = \"preposition\"\n    CONJUNCTION = \"conjunction\"\n    PRONOUN = \"pronoun\"\n    INTERJECTION = \"interjection\"\n    NUMERAL = \"numeral\"\n    PARTICLE = \"particle\"\n    ARTICLE = \"article\"  # Greek only\n    UNKNOWN = \"unknown\"\n```\n\n### Source POS Mappings\n\n| Source | Code | Normalized |\n|--------|------|------------|\n| Whitaker's | N | NOUN |\n| Whitaker's | V | VERB |\n| Whitaker's | ADJ | ADJECTIVE |\n| Whitaker's | PRON | PRONOUN |\n| Lewis \u0026 Short | \"noun\" | NOUN |\n| Lewis \u0026 Short | \"verb\" | VERB |\n| LSJ | \"noun\" | NOUN |\n| Morpheus | \"verb\" | VERB |\n\n## Gender Standardization\n\n```python\nclass Gender(Enum):\n    MASCULINE = \"m\"\n    FEMININE = \"f\"\n    NEUTER = \"n\"\n    COMMON = \"c\"  # Masculine or feminine (Latin: civis, Greek: ὁ/ἡ θεός)\n    UNKNOWN = \"x\"\n```\n\n## Normalizer Responsibilities\n\nEach source-specific normalizer must:\n\n1. **Reconstruct headword** - From stems/roots to full dictionary form\n2. **Map POS** - Source-specific codes to PartOfSpeech enum\n3. **Extract gender** - For nominals, standardize to Gender enum\n4. **Format genitive** - Consistent \"-ending\" format\n5. **Extract principal parts** - For verbs, in canonical order\n6. **Clean senses** - Remove citations, abbreviations, markup\n7. **Set confidence** - Based on match quality\n\n## Benefits\n\n1. **Single source of truth** for normalization logic\n2. **Testable in isolation** - unit test each normalizer\n3. **Easy to add sources** - implement Extractor + Normalizer\n4. **Data quality metrics** - track confidence, source distribution\n5. **Intelligent merging** - combine data from multiple sources\n6. **Consistent output** - rendering code never sees source differences\n\n## Sub-Issues\n\n1. Define core data models (NormalizedLexicalEntry, enums)\n2. Implement Latin normalizers (Whitaker's, Lewis \u0026 Short, APIs)\n3. Implement Greek normalizers (LSJ, Morpheus, CLTK)\n4. Refactor lexicon classes to use normalization layer\n5. Update Gloss model to consume NormalizedLexicalEntry\n6. Add confidence scoring and source tracking\n7. Migrate existing headword reconstruction logic\n\n## Success Criteria\n\n- [ ] All dictionary lookups go through normalization layer\n- [ ] Headword reconstruction works for all Latin word types\n- [ ] Headword reconstruction works for all Greek word types\n- [ ] Source provenance tracked for every entry\n- [ ] Unit tests for each normalizer\n- [ ] No regression in definition quality\n- [ ] Missing definitions report includes source attempted","status":"open","priority":1,"issue_type":"feature","created_at":"2026-01-11T10:10:13.777591-06:00","updated_at":"2026-01-11T10:10:13.777591-06:00","dependencies":[{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-4d5","type":"blocks","created_at":"2026-01-11T10:15:17.62558-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-m7s","type":"blocks","created_at":"2026-01-11T10:15:17.783972-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-8g3","type":"blocks","created_at":"2026-01-11T10:15:17.941724-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-alf","type":"blocks","created_at":"2026-01-11T10:15:18.101928-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-rhm","type":"blocks","created_at":"2026-01-11T10:15:18.262542-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-3di","type":"blocks","created_at":"2026-01-11T10:15:18.430343-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-k4f","depends_on_id":"auto-commentary-ruq","type":"blocks","created_at":"2026-01-11T10:15:18.59554-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-m7s","title":"Implement Latin Whitaker's Words normalizer","description":"## Task\n\nCreate a normalizer that converts raw Whitaker's Words output to NormalizedLexicalEntry.\n\n## Location\n\n`autocom/languages/latin/normalizers/whitakers.py`\n\n## Interface\n\n```python\nfrom typing import Optional, Any\nfrom autocom.core.lexical import NormalizedLexicalEntry\n\nclass WhitakersNormalizer:\n    \"\"\"Normalizes Whitaker's Words output to canonical form.\"\"\"\n    \n    def normalize(self, raw_result: Any, query_word: str) -\u003e Optional[NormalizedLexicalEntry]:\n        \"\"\"Convert Whitaker's parse result to normalized entry.\n        \n        Args:\n            raw_result: Output from whitakers_words Parser.parse()\n            query_word: The original word that was looked up\n            \n        Returns:\n            NormalizedLexicalEntry if successful, None if no valid data\n        \"\"\"\n        ...\n```\n\n## Key Responsibilities\n\n### 1. Headword Reconstruction (THE BUG FIX)\n\nThis is where we fix auto-commentary-98t. Map (wordType, declension/conjugation, gender) to nominative ending:\n\n```python\n# Noun nominative endings: (declension, gender, is_plural_tantum) -\u003e ending\nNOUN_NOM_ENDINGS = {\n    (1, Gender.FEMININE, False): \"a\",      # terra\n    (1, Gender.MASCULINE, False): \"a\",     # poeta\n    (2, Gender.MASCULINE, False): \"us\",    # dominus\n    (2, Gender.NEUTER, False): \"um\",       # bellum\n    (2, Gender.NEUTER, True): \"a\",         # arma (pl. tantum)\n    (3, Gender.MASCULINE, False): \"\",      # varies - use raw\n    (3, Gender.FEMININE, False): \"\",       # varies - use raw\n    (3, Gender.NEUTER, False): \"\",         # varies - use raw\n    (4, Gender.MASCULINE, False): \"us\",    # eventus\n    (4, Gender.NEUTER, False): \"u\",        # cornu\n    (5, Gender.FEMININE, False): \"es\",     # res\n}\n\n# Adjective nominative endings: declension -\u003e masc. nom. sg.\nADJ_NOM_ENDINGS = {\n    1: \"us\",   # 1st/2nd decl: bonus, multus\n    2: \"us\",   # variant coding\n    3: \"\",     # 3rd decl: varies (omnis, felix, acer)\n}\n\n# Pronoun headwords: stem -\u003e dictionary form\nPRON_HEADWORDS = {\n    \"ill\": \"ille\",\n    \"hic\": \"hic\", \"h\": \"hic\",\n    \"is\": \"is\", \"e\": \"is\",\n    \"qui\": \"qui\", \"qu\": \"qui\",\n    \"quis\": \"quis\",\n    \"ips\": \"ipse\",\n    \"idem\": \"idem\", \"eid\": \"idem\",\n    \"ali\": \"alius\",\n    \"alt\": \"alter\",\n    \"null\": \"nullus\",\n    \"tot\": \"totus\",\n    \"sol\": \"solus\",\n}\n```\n\n### 2. POS Mapping\n\n```python\nWHITAKERS_POS_MAP = {\n    \"N\": PartOfSpeech.NOUN,\n    \"V\": PartOfSpeech.VERB,\n    \"ADJ\": PartOfSpeech.ADJECTIVE,\n    \"ADV\": PartOfSpeech.ADVERB,\n    \"PREP\": PartOfSpeech.PREPOSITION,\n    \"CONJ\": PartOfSpeech.CONJUNCTION,\n    \"PRON\": PartOfSpeech.PRONOUN,\n    \"INTERJ\": PartOfSpeech.INTERJECTION,\n    \"NUM\": PartOfSpeech.NUMERAL,\n    \"VPAR\": PartOfSpeech.VERB,  # Verbal participle\n    \"SUPINE\": PartOfSpeech.VERB,\n    \"PACK\": PartOfSpeech.UNKNOWN,\n}\n```\n\n### 3. Gender Extraction\n\n```python\nWHITAKERS_GENDER_MAP = {\n    \"M\": Gender.MASCULINE,\n    \"F\": Gender.FEMININE,\n    \"N\": Gender.NEUTER,\n    \"C\": Gender.COMMON,\n    \"X\": Gender.UNKNOWN,\n}\n```\n\n### 4. Sense Cleaning\n\nRemove editorial brackets like `[a puere =\u003e from boyhood]` and clean punctuation.\n\n### 5. Principal Parts Construction\n\nFor verbs, construct from roots array:\n- roots[0] + conjugation ending = present (headword)\n- roots[2] + \"ī\" = perfect\n- roots[3] + \"um\" = supine\n\n### 6. Genitive Construction\n\nFor nouns, use DECLENSION_GENITIVE_MAP:\n```python\nDECLENSION_GENITIVE_MAP = {1: \"-ae\", 2: \"-ī\", 3: \"-is\", 4: \"-ūs\", 5: \"-ēī\"}\n```\n\n## Test Cases\n\n`tests/languages/latin/normalizers/test_whitakers.py`:\n\n- Noun 1st decl F: \"terra\" from stem \"terr\"\n- Noun 2nd decl M: \"dominus\" from stem \"domin\"\n- Noun 2nd decl N pl.tantum: \"arma\" from stem \"arm\"\n- Adjective 1st/2nd: \"multus\" from stem \"mult\"\n- Adjective 3rd: \"omnis\", \"felix\"\n- Pronoun: \"ille\" from stem \"ill\"\n- Verb 1st conj: \"amo\" from stem \"am\"\n- Proper noun: \"Italia\" from stem \"itali\"\n\n## Acceptance Criteria\n\n- [ ] All headword reconstruction patterns implemented\n- [ ] POS correctly mapped for all Whitaker's types\n- [ ] Gender correctly extracted\n- [ ] Senses cleaned of editorial markup\n- [ ] Principal parts formatted correctly\n- [ ] All test cases pass\n- [ ] Fixes the truncated headword bug","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-11T10:12:12.628041-06:00","updated_at":"2026-01-11T11:13:14.618095-06:00","closed_at":"2026-01-11T11:13:14.618095-06:00","close_reason":"Implemented WhitakersNormalizer with comprehensive headword reconstruction, POS/gender mapping, principal parts extraction, sense cleaning, and 60 tests","dependencies":[{"issue_id":"auto-commentary-m7s","depends_on_id":"auto-commentary-4d5","type":"blocks","created_at":"2026-01-11T10:15:16.490128-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-n18","title":"Fix glossary entry format to match Steadman style (headword with ending, e.g. voco, -are)","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-09T16:59:01.55466-06:00","updated_at":"2026-01-09T16:59:01.55466-06:00"}
{"id":"auto-commentary-nmj","title":"Add adjective nominative ending reconstruction","description":"## Task\n\nAdd ADJ_NOM_ENDING_MAP to reconstruct nominative singular masculine headwords for adjectives from Whitaker's stems.\n\n## Mapping Logic\n\nLatin dictionary convention: adjectives use masculine nominative singular as headword.\n\n```python\n# declension -\u003e masculine nominative ending\nADJ_NOM_ENDING_MAP = {\n    1: 'us',      # 1st/2nd decl: multus, bonus, saevus, primus\n    2: 'us',      # Same as 1: some coded as 2\n    3: 'is',      # 3rd decl two-termination: omnis, brevis, fortis\n    # Note: 3rd decl one/three termination need special handling\n}\n\n# Special 3rd declension patterns (check if stem ends in certain patterns)\nADJ_3RD_DECL_SPECIAL = {\n    # One-termination: stem IS the headword (felix, audax, vetus)\n    # Three-termination: -er/-ris/-re pattern (acer, acris, acre)\n}\n```\n\n## Implementation\n\nIn `_lookup_whitaker_with_metadata`:\n\n```python\nif wt_name == 'ADJ':\n    decl = category[0] if category else None\n    ending = ADJ_NOM_ENDING_MAP.get(decl, 'us')  # Default to -us\n    result['headword'] = f'{stem}{ending}'\n```\n\n## Test Cases\n\n- saev + us = saevus (1st/2nd decl)\n- mult + us = multus (1st/2nd decl)\n- prim + us = primus (1st/2nd decl)\n- omn + is = omnis (3rd decl two-term)\n- felix (one-term: stem = headword)\n- acer (three-term: needs -er ending)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-11T10:05:04.007718-06:00","updated_at":"2026-01-11T10:05:04.007718-06:00"}
{"id":"auto-commentary-rhm","title":"Refactor lexicon classes to use normalization layer","description":"## Task\n\nRefactor `LatinLexicon` and `GreekLexicon` to use the new normalization layer instead of inline transformation logic.\n\n## Current State\n\n`autocom/languages/latin/lexicon.py`:\n- `_lookup_whitaker_with_metadata()` - does extraction AND normalization inline\n- `lookup_with_metadata()` - Lewis \u0026 Short extraction AND normalization inline\n- `enrich_token()` - merges results and builds Gloss\n\n## Target State\n\n```python\nclass LatinLexicon:\n    def __init__(self, ...):\n        # Extractors (raw data)\n        self._whitakers_extractor = WhitakersExtractor()\n        self._lewis_short_extractor = LewisShortExtractor()\n        \n        # Normalizers (raw -\u003e canonical)\n        self._whitakers_normalizer = WhitakersNormalizer()\n        self._lewis_short_normalizer = LewisShortNormalizer()\n        \n    def lookup(self, lemma: str) -\u003e Optional[NormalizedLexicalEntry]:\n        \"\"\"Look up lemma and return normalized entry.\"\"\"\n        # Try Whitaker's first\n        raw = self._whitakers_extractor.extract(lemma)\n        if raw:\n            entry = self._whitakers_normalizer.normalize(raw, lemma)\n            if entry and entry.senses:\n                return entry\n        \n        # Fall back to Lewis \u0026 Short\n        raw = self._lewis_short_extractor.extract(lemma)\n        if raw:\n            entry = self._lewis_short_normalizer.normalize(raw, lemma)\n            if entry and entry.senses:\n                return entry\n        \n        # Try APIs...\n        return None\n    \n    def enrich_token(self, token: Token, frequency: int = None) -\u003e Token:\n        \"\"\"Enrich token using normalized lookup.\"\"\"\n        lemma = token.analysis.lemma if token.analysis else token.text\n        entry = self.lookup(lemma)\n        \n        if entry:\n            token.gloss = Gloss.from_normalized_entry(entry, frequency)\n        else:\n            token.gloss = Gloss(lemma=lemma, senses=[])\n        \n        return token\n```\n\n## Refactoring Steps\n\n### Phase 1: Extract Extractors\n\nMove raw data extraction to separate classes:\n1. `WhitakersExtractor` - wraps whitakers_words Parser\n2. `LewisShortExtractor` - loads and queries L\u0026S JSON files\n3. `WordNetAPIExtractor` - queries Latin WordNet API\n4. `LatinSimpleExtractor` - queries Latin is Simple API\n\n### Phase 2: Wire Up Normalizers\n\nConnect extractors to normalizers:\n```python\nraw_whitakers = self._whitakers_extractor.extract(lemma)\nnormalized = self._whitakers_normalizer.normalize(raw_whitakers, lemma)\n```\n\n### Phase 3: Update Lookup Flow\n\nReplace inline logic with extractor + normalizer calls.\n\n### Phase 4: Update Gloss Creation\n\nAdd `Gloss.from_normalized_entry()` factory method.\n\n### Phase 5: Remove Old Code\n\nDelete the inline normalization code from:\n- `_lookup_whitaker_with_metadata()`\n- `lookup_with_metadata()`\n- `_extract_dictionary_metadata()`\n\n## Backward Compatibility\n\nMaintain same public interface:\n- `lexicon.enrich(lines)`\n- `lexicon.enrich_token(token)`\n- `lexicon.lookup(lemma)`\n\n## Test Strategy\n\n1. Create integration tests that verify same output before/after\n2. Run existing tests to ensure no regression\n3. Add new unit tests for extractors and normalizers\n\n## Acceptance Criteria\n\n- [ ] Extractors separated from normalizers\n- [ ] All lookups go through normalization layer\n- [ ] Existing tests pass (no regression)\n- [ ] Gloss model works with normalized entries\n- [ ] Code is cleaner and more maintainable","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-11T10:13:14.730457-06:00","updated_at":"2026-01-11T10:13:14.730457-06:00","dependencies":[{"issue_id":"auto-commentary-rhm","depends_on_id":"auto-commentary-m7s","type":"blocks","created_at":"2026-01-11T10:15:17.135195-06:00","created_by":"daemon"},{"issue_id":"auto-commentary-rhm","depends_on_id":"auto-commentary-8g3","type":"blocks","created_at":"2026-01-11T10:15:17.296957-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-ruq","title":"Add confidence scoring and source tracking","description":"## Task\n\nImplement confidence scoring for dictionary lookups and comprehensive source tracking for data provenance.\n\n## Confidence Scoring\n\n### Score Levels\n\n```python\nclass ConfidenceLevel:\n    EXACT_MATCH = 1.0      # Lemma found exactly as queried\n    VARIANT_MATCH = 0.9    # Found via u/v or i/j variant\n    STEM_MATCH = 0.7       # Found via morphological stem guessing\n    FUZZY_MATCH = 0.5      # Found via fuzzy/approximate matching\n    API_FALLBACK = 0.6     # Found via API (may be less reliable)\n    NOT_FOUND = 0.0        # No definition found\n```\n\n### Scoring Logic\n\n```python\ndef calculate_confidence(\n    query_lemma: str,\n    found_lemma: str,\n    match_type: str,\n    source: str,\n) -\u003e float:\n    \"\"\"Calculate confidence score for a dictionary match.\"\"\"\n    base_score = {\n        \"exact\": 1.0,\n        \"variant\": 0.9,\n        \"stem\": 0.7,\n        \"alternative\": 0.6,\n        \"fuzzy\": 0.5,\n    }.get(match_type, 0.5)\n    \n    # Adjust for source reliability\n    source_modifier = {\n        \"whitakers\": 1.0,      # High quality, curated\n        \"lewis_short\": 1.0,    # Scholarly standard\n        \"lsj\": 1.0,            # Scholarly standard\n        \"wordnet_api\": 0.9,    # Good but may have gaps\n        \"simple_api\": 0.8,     # Convenient but less scholarly\n        \"morpheus\": 0.9,       # Good for Greek\n    }.get(source, 0.7)\n    \n    return min(base_score * source_modifier, 1.0)\n```\n\n## Source Tracking\n\n### Source Identifiers\n\n```python\nSOURCES = {\n    \"whitakers\": \"Whitaker's Words\",\n    \"lewis_short\": \"Lewis \u0026 Short Latin Dictionary\",\n    \"lsj\": \"Liddell-Scott-Jones Greek Lexicon\",\n    \"wordnet_api\": \"Latin WordNet API\",\n    \"simple_api\": \"Latin is Simple API\",\n    \"morpheus\": \"Perseus Morpheus\",\n    \"cltk\": \"CLTK Lemmatizer\",\n}\n```\n\n### Tracking in NormalizedLexicalEntry\n\nAlready included in the model:\n```python\nsource: str = Field(..., description=\"Dictionary source identifier\")\nconfidence: float = Field(1.0, ge=0.0, le=1.0, description=\"Match quality score\")\n```\n\n## Enhanced Missing Definitions Report\n\nUpdate `collect_missing_definitions()` to include:\n\n```python\n{\n    \"word\": \"unknown\",\n    \"lemma\": \"unknown\",\n    \"page\": 1,\n    \"line_number\": 42,\n    \"context\": \"line text...\",\n    \"sources_tried\": [\"whitakers\", \"lewis_short\", \"wordnet_api\"],\n    \"best_confidence\": 0.0,\n    \"suggestion\": \"May be proper noun or rare word\",\n}\n```\n\n## Analytics\n\nAdd method to lexicon for lookup statistics:\n\n```python\ndef get_lookup_analytics(self) -\u003e Dict[str, Any]:\n    \"\"\"Get statistics about dictionary lookups.\"\"\"\n    return {\n        \"total_lookups\": self._stats[\"total\"],\n        \"by_source\": {\n            \"whitakers\": self._stats[\"whitakers\"],\n            \"lewis_short\": self._stats[\"lewis_short\"],\n            ...\n        },\n        \"confidence_distribution\": {\n            \"high (\u003e0.9)\": count,\n            \"medium (0.6-0.9)\": count,\n            \"low (\u003c0.6)\": count,\n        },\n        \"not_found\": self._stats[\"not_found\"],\n    }\n```\n\n## CLI Integration\n\nAdd analytics to commentary output:\n\n```\n2026-01-11 10:00:00 | INFO | Lookup analytics:\n  - Total lookups: 1,234\n  - Whitaker's: 1,100 (89%)\n  - Lewis \u0026 Short: 80 (6%)\n  - API fallbacks: 30 (2%)\n  - Not found: 24 (2%)\n  - Average confidence: 0.94\n```\n\n## Test Cases\n\n- Exact match gets 1.0 confidence\n- u/v variant gets 0.9 confidence\n- Stem-based lookup gets lower confidence\n- Source correctly recorded\n- Analytics accurate\n\n## Acceptance Criteria\n\n- [ ] Confidence scoring implemented\n- [ ] Source tracking in all entries\n- [ ] Enhanced missing definitions report\n- [ ] Analytics method available\n- [ ] CLI shows lookup stats\n- [ ] Test coverage for scoring logic","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-11T10:13:56.130319-06:00","updated_at":"2026-01-11T10:13:56.130319-06:00","dependencies":[{"issue_id":"auto-commentary-ruq","depends_on_id":"auto-commentary-rhm","type":"blocks","created_at":"2026-01-11T10:15:17.465632-06:00","created_by":"daemon"}]}
{"id":"auto-commentary-th9","title":"Show first-occurrence line number in glossary instead of frequency count","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-09T16:59:01.272811-06:00","updated_at":"2026-01-09T16:59:01.272811-06:00"}
{"id":"auto-commentary-ucm","title":"In the latin commentary, it appears that the ending of words in the dictionary section are missing for example on page 1 we have ag instead of ago (Im referencing commentary.pdf","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T16:42:00.809808-06:00","updated_at":"2026-01-09T16:51:33.614971-06:00","closed_at":"2026-01-09T16:51:33.614971-06:00","close_reason":"Fixed verb headword construction in lexicon.py. Added CONJUGATION_ENDING_MAP to properly append 1st person singular endings to verb stems (ag- → ago, fac- → facio)."}
{"id":"auto-commentary-vj9","title":"Implement persistent caching for dictionary lookups (Whitaker's + APIs) using existing SQLite cache infrastructure","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-09T17:04:24.679732-06:00","updated_at":"2026-01-11T09:32:14.799749-06:00","closed_at":"2026-01-11T09:32:14.799749-06:00","close_reason":"Implemented persistent SQLite-based dictionary cache for Whitaker's Words and API lookups. Benchmarks show 7.5x speedup for cached lookups with 90% hit rate for repeated words. Added 22 tests covering cache operations, statistics, expiration, and persistence."}
